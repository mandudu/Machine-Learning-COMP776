{"cells":[{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Exg-Tc3qZ3HNvnl2yvndBkmAnLq6glWd"},"id":"R0K9YELKzY_q","outputId":"65ebdce9-9b60-4054-c474-09629fb2a5ee","executionInfo":{"status":"ok","timestamp":1677196422626,"user_tz":300,"elapsed":23601,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import cv2\n","import numpy as np\n","import scipy.ndimage\n","from scipy.ndimage.morphology import distance_transform_edt\n","from google.colab.patches import cv2_imshow\n","\n","def MainFunction(JPEGimage, image, number=''):\n","    img_ = cv2.imread(JPEGimage)\n","    img1 = cv2.cvtColor(img_,cv2.COLOR_BGR2GRAY)\n","\n","    img = cv2.imread(image)\n","    img2 = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n","\n","    sift = cv2.xfeatures2d.SIFT_create()\n","    # find the key points and descriptors with SIFT\n","    kp1, des1 = sift.detectAndCompute(img1,None)\n","    kp2, des2 = sift.detectAndCompute(img2,None)\n","\n","    img1_result = cv2.drawKeypoints(img_,kp1,None, cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","    cv2_imshow(img1_result)\n","\n","    match = cv2.BFMatcher(cv2.NORM_L2, True)\n","    matches = match.match(des1,des2)\n","    matches = sorted(matches, key = lambda x:x.distance)\n","    res = cv2.drawMatches(img_,kp1,img,kp2,matches[:10],None,flags=2)\n","    cv2_imshow(res)\n","            \n","    pts1 = np.array([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1, 2)\n","    pts2 = np.array([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1, 2)\n","    M, _ = cv2.findHomography(pts2, pts1, cv2.RANSAC) \n","    \n","    warped_image = cv2.warpPerspective(img ,M,(img_.shape[1] + img.shape[1], img_.shape[0]))\n","    warped_image = warped_image/2\n","    warped_image[0:img_.shape[0], 0: img_.shape[1]] += img_/2\n","    cv2.imwrite(\"not_blended.png\", warped_image)\n","    warped_image = cv2.warpPerspective(img, M, (img_.shape[1] + img.shape[1], img_.shape[0]))\n","    warped_image[0:img_.shape[0], 0:img_.shape[1]] = img_\n","    cv2.imwrite(\"blended.png\", warped_image)\n","  \n","    warped_image[0:img.shape[0], 0:img.shape[1]] = img_\n","    warped_image = warped_image\n","    warped_image = cv2.cvtColor(warped_image,cv2.COLOR_BGR2RGBA) \n","  \n","    cv2_imshow(warped_image)\n","\n","    warped_image2 = cv2.warpPerspective(img, M, (img_.shape[1] + img.shape[1], img_.shape[0]))\n","    warped_image1 = np.zeros(warped_image2.shape)\n","    warped_image1[0:img_.shape[0], 0:img_.shape[1]] = img_\n","    distance1 = distance_transform_edt(warped_image1)\n","    distance2 = distance_transform_edt(warped_image2)\n","\n","    w1 = distance1 / np.maximum((distance1+distance2),1)\n","    w2 = distance2 / np.maximum((distance1+distance2),1)\n","    warped_image = warped_image1*w1 + warped_image2*w2\n","    warped_image = warped_image[0:warped_image.shape[0], 0: warped_image.shape[1]//5]\n","    cv2_imshow(warped_image)\n","    cv2.imwrite(\"self_final.png\", warped_image)\n","    print(warped_image.shape)\n","\n","def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n","    # initialize the dimensions of the image to be resized and\n","    # grab the image size\n","    dim = None\n","    (h, w) = image.shape[:2]\n","\n","    # if both the width and height are None, then return the\n","    # original image\n","    if width is None and height is None:\n","        return image\n","\n","    # check to see if the width is None\n","    if width is None:\n","        # calculate the ratio of the height and construct the\n","        # dimensions\n","        r = height / float(h)\n","        dim = (int(w * r), height)\n","\n","    # otherwise, the height is None\n","    else:\n","        # calculate the ratio of the width and construct the\n","        # dimensions\n","        r = width / float(w)\n","        dim = (width, int(h * r))\n","\n","    # resize the image\n","    resized = cv2.resize(image, dim, interpolation = inter)\n","\n","    # return the resized image\n","    return resized\n","\n","img1 = cv2.imread('/content/img1.jpg')\n","img2 = cv2.imread('/content/img2.jpg')\n","img3 = cv2.imread('/content/img3.jpg')\n","img4 = cv2.imread('/content/img4.jpg')\n","\n","imgList = [img1, img2, img3, img4] \n","\n","for i in range(4):\n","  resize = image_resize(imgList[i], imgList[i].shape[0]//3, imgList[i].shape[1]//3)\n","  cv2.imwrite(\"resize_img%d.jpg\"%(i), resize)  \n","    \n","\n","\n","MainFunction('/content/self4.png', '/content/self5.png')\n"]},{"cell_type":"markdown","source":["Reasonings:\n","\n","I took my photo outdoor since there are more patterns and dynamic points that could overlap with other photos. Moreover, I thought it would be a lot harder to validate my model with indoor photos that contains plane walls with small characteristics since the model would work perfectly anytime, which would be difficult to improve upon our model. \n","\n","The first flaw that I encountered with the first attempt of stitching my own photos was that the model would not generate panorama photos that had different angles. For instance, when I was taking a photo of our campus just by altering my wrist angles, the model that I implemented has produced a photo that was extremely inaccurate. However, like the model worked fine when I was moving sideways with still hand positions (like the panorama photo that I generated as my final_self.png. \n","\n","Another flaw that I encountered was that the panorama photos generated by the model had some issues towards the right side of the image. It seems like the photo has stretched out to the right. I also believe that this was caused by different angles between the photos, and possibly from the floor level being different. \n","\n","Lastly, if we look at final_self.png photo, it shows that the trees and plants in the middle part is blurred out, and they are not in the right position compared to the original photos. I believe this was also due to the difference in the elevation of the floor in img4 and img3. Therefore, the model has possibly taken the average homographic transformation within these two planes.\n","\n","In order to make my model more robust, there would need to be a function that aligns the images accordingly to its angles. We could possibly use convNets to align our images by performing corners and edge dectections to align the two images. Moreover, we could use spatial transformer Networks to train and align two images by learning the transformation parameters taht map one image onto the other. \n","\n"],"metadata":{"id":"aQemgzvKiLqU"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyObCGS634SKvNohB6NxJOQt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}